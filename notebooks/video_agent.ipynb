{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-14T17:49:16.194249Z",
     "start_time": "2024-10-14T17:49:16.188498Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "from abc import ABC\n",
    "import renderlab\n",
    "import copy\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "from policies import LinearGaussianPolicy, LinearPolicy\n",
    "from envs.utils import ActionBoundsIdx"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T17:49:16.433835Z",
     "start_time": "2024-10-14T17:49:16.429221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(env, policy):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param env: (Env object) the Gym environment\n",
    "    :param policy: (BasePolicy object) the policy in stable_baselines3\n",
    "    :param gamma: (float) the discount factor\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    frames = []\n",
    "    \n",
    "    #while not done: # iterate over the steps until termination\n",
    "    for _ in range (1000):\n",
    "        action = policy.draw_action(obs)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_rewards.append(reward) # compute discounted reward\n",
    "        \n",
    "        frames.append(env.render())\n",
    "\n",
    "    mean_episode_reward = np.mean(episode_rewards)\n",
    "    std_episode_reward = np.std(episode_rewards)\n",
    "    print(\"Mean reward:\", mean_episode_reward,\n",
    "          \"Std reward:\", std_episode_reward)\n",
    "\n",
    "    return frames"
   ],
   "id": "6da1aa17333096f4",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T17:49:16.740573Z",
     "start_time": "2024-10-14T17:49:16.738077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_video(source, fps=60, output_name='/Users/leonardo/Desktop/Thesis/MagicRL/videos'):\n",
    "    out = cv2.VideoWriter(output_name + '.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (source[0].shape[1], source[0].shape[0]))\n",
    "    for i in range(len(source)):\n",
    "        out.write(source[i])\n",
    "    out.release()"
   ],
   "id": "ae019ede08085964",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T17:49:17.129211Z",
     "start_time": "2024-10-14T17:49:17.122970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CostHopper(gym.Env, ABC):\n",
    "    \"\"\"Hopper environment with added cost functionality.\"\"\"\n",
    "    metadata = {\"render.modes\": [\"rgb_array\"], \"video.frames_per_second\": 24}\n",
    "    def __init__(\n",
    "        self, horizon: int = 0, gamma: float = 0.99, verbose: bool = False, clip: bool = False, render_mode: str = None\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes the CostHopper environment.\"\"\"\n",
    "        # Initialize base attributes\n",
    "        self.state = None\n",
    "        self.horizon = horizon\n",
    "        assert 0 <= gamma <= 1, \"[ERROR] Invalid Discount Factor value.\"\n",
    "        self.gamma = gamma\n",
    "        self.time = 0\n",
    "        self.verbose = verbose\n",
    "        self.state_dim = 0\n",
    "        self.action_dim = 0\n",
    "        self.clip = clip\n",
    "        self.with_costs = True\n",
    "        self.how_many_costs = 1\n",
    "        self.continuous_env = True\n",
    "\n",
    "        # Set render mode and gym environment\n",
    "        self.render_mode = render_mode\n",
    "        self.gym_env = gym.make('Hopper-v4', render_mode=render_mode)\n",
    "        self.action_bounds = [-1, 1]\n",
    "        self.state_dim = self.gym_env.observation_space.shape[0]    # 11\n",
    "        self.action_dim = self.gym_env.action_space.shape[0]        # 3\n",
    "        self.action_space = self.gym_env.action_space\n",
    "        self.observation_space = self.gym_env.observation_space\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Takes an action in the environment and computes the cost.\"\"\"\n",
    "        if self.clip:\n",
    "            clipped_action = np.clip(\n",
    "                action,\n",
    "                self.action_bounds[ActionBoundsIdx.lb],\n",
    "                self.action_bounds[ActionBoundsIdx.ub],\n",
    "                dtype=np.float64\n",
    "            )\n",
    "        else:\n",
    "            clipped_action = action\n",
    "\n",
    "        slack = action - clipped_action\n",
    "        cost = np.linalg.norm(slack) if slack.any() > 0 else 0\n",
    "\n",
    "        obs, reward, done, _ , _ = self.gym_env.step(clipped_action)\n",
    "        self.state = copy.deepcopy(obs)\n",
    "        info = {\"costs\": np.array([cost], dtype=np.float64)}\n",
    "\n",
    "        return obs, reward, done, None, info\n",
    "\n",
    "    def reset(self, seed = None, options = None):\n",
    "        \"\"\"Resets the environment.\"\"\"\n",
    "        obs = self.gym_env.reset()\n",
    "        self.state = copy.deepcopy(obs[0])\n",
    "        return obs\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Renders the environment.\"\"\"\n",
    "        return self.gym_env.render()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the environment.\"\"\"\n",
    "        self.gym_env.close()\n",
    "\n",
    "    def sample_action(self):\n",
    "        \"\"\"Samples a random action from the action space.\"\"\"\n",
    "        return self.gym_env.action_space.sample()\n",
    "\n",
    "    def sample_state(self, args: dict = None):\n",
    "        \"\"\"Samples a random state from the observation space.\"\"\"\n",
    "        return self.gym_env.observation_space.sample()\n",
    "\n",
    "    def set_state(self, state):\n",
    "        \"\"\"Sets the state of the environment.\"\"\"\n",
    "        self.state = state\n",
    "\n",
    "register(\n",
    "    id='CostHopper-v4',\n",
    "    entry_point='__main__:CostHopper',\n",
    "    max_episode_steps=100,\n",
    "    kwargs={'render_mode': 'rgb_array'}, \n",
    ")"
   ],
   "id": "18e550201f96e333",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T17:49:17.778542Z",
     "start_time": "2024-10-14T17:49:17.700245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CostSwimmer(gym.Env, ABC):\n",
    "    \"\"\"Hopper environment with added cost functionality.\"\"\"\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 24}\n",
    "    def __init__(\n",
    "        self, horizon: int = 0, gamma: float = 0.99, verbose: bool = False, clip: bool = False, render_mode: str = None\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes the CostHopper environment.\"\"\"\n",
    "        # Initialize base attributes\n",
    "        self.state = None\n",
    "        self.horizon = horizon\n",
    "        assert 0 <= gamma <= 1, \"[ERROR] Invalid Discount Factor value.\"\n",
    "        self.gamma = gamma\n",
    "        self.time = 0\n",
    "        self.verbose = verbose\n",
    "        self.state_dim = 0\n",
    "        self.action_dim = 0\n",
    "        self.clip = clip\n",
    "        self.with_costs = True\n",
    "        self.how_many_costs = 1\n",
    "        self.continuous_env = True\n",
    "\n",
    "        # Set render mode and gym environment\n",
    "        self.render_mode = render_mode\n",
    "        self.gym_env = gym.make('Swimmer-v4', render_mode=render_mode)\n",
    "        self.action_bounds = [-1, 1]\n",
    "        self.state_dim = self.gym_env.observation_space.shape[0]    # 11\n",
    "        self.action_dim = self.gym_env.action_space.shape[0]        # 3\n",
    "        self.action_space = self.gym_env.action_space\n",
    "        self.observation_space = self.gym_env.observation_space\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Takes an action in the environment and computes the cost.\"\"\"\n",
    "        if self.clip:\n",
    "            clipped_action = np.clip(\n",
    "                action,\n",
    "                self.action_bounds[ActionBoundsIdx.lb],\n",
    "                self.action_bounds[ActionBoundsIdx.ub],\n",
    "                dtype=np.float64\n",
    "            )\n",
    "        else:\n",
    "            clipped_action = action\n",
    "\n",
    "        slack = action - clipped_action\n",
    "        cost = np.linalg.norm(slack) if slack.any() > 0 else 0\n",
    "\n",
    "        obs, reward, done, _ , _ = self.gym_env.step(clipped_action)\n",
    "        self.state = copy.deepcopy(obs)\n",
    "        info = {\"costs\": np.array([cost], dtype=np.float64)}\n",
    "\n",
    "        return obs, reward, done, None, info\n",
    "\n",
    "    def reset(self, seed = None, options = None):\n",
    "        \"\"\"Resets the environment.\"\"\"\n",
    "        obs = self.gym_env.reset()\n",
    "        self.state = copy.deepcopy(obs[0])\n",
    "        return obs\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Renders the environment.\"\"\"\n",
    "        return self.gym_env.render()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the environment.\"\"\"\n",
    "        self.gym_env.close()\n",
    "\n",
    "    def sample_action(self):\n",
    "        \"\"\"Samples a random action from the action space.\"\"\"\n",
    "        return self.gym_env.action_space.sample()\n",
    "\n",
    "    def sample_state(self, args: dict = None):\n",
    "        \"\"\"Samples a random state from the observation space.\"\"\"\n",
    "        return self.gym_env.observation_space.sample()\n",
    "\n",
    "    def set_state(self, state):\n",
    "        \"\"\"Sets the state of the environment.\"\"\"\n",
    "        self.state = state\n",
    "\n",
    "register(\n",
    "    id='CostSwimmer-v4',\n",
    "    entry_point='__main__:CostSwimmer',\n",
    "    max_episode_steps=100,\n",
    "    kwargs={'render_mode': 'rgb_array'}, \n",
    ")\n"
   ],
   "id": "261ae9d1ad6b2c18",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T17:49:24.768825Z",
     "start_time": "2024-10-14T17:49:24.741235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env_eval = gym.make('CostHopper-v4', render_mode = \"rgb_array\")\n",
    "# env_eval = renderlab.RenderFrame(env_eval, \"/Users/leonardo/Desktop/Thesis/MagicRL/videos\")"
   ],
   "id": "9c65a64a75c9bb52",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T17:49:25.124841Z",
     "start_time": "2024-10-14T17:49:25.122196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import the json with the optimal policy\n",
    "\n",
    "# Swimmer\n",
    "\"\"\"\n",
    "with open(\"/Users/leonardo/Desktop/Thesis/Data/cpgpe_3000_swimmer_300_adam_p0001_d001_linear_batch_100_reg_00001_risk_tc_p16_var_001_a/cpgpe_3000_swimmer_300_adam_p0001_d001_linear_batch_100_reg_00001_risk_tc_p16_var_001_a_trial_0/cpgpe_results.json\", \"r\") as read_file:\n",
    "    optimal_policy = json.load(read_file)\n",
    "\"\"\"\n",
    "# Hopper\n",
    "with open(\"/Users/leonardo/Desktop/Thesis/Data/cpgpe_3000_hopper_300_adam_p001_d01_linear_batch_100_reg_00001_risk_tc_p33_var_01_a/cpgpe_3000_hopper_300_adam_p001_d01_linear_batch_100_reg_00001_risk_tc_p33_var_01_a_trial_0/cpgpe_results.json\", \"r\") as read_file:\n",
    "    optimal_policy = json.load(read_file)\n"
   ],
   "id": "9615b6f19b96d46c",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T17:49:30.984470Z",
     "start_time": "2024-10-14T17:49:30.982320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get the optimal policy\n",
    "best_thetas = np.array(optimal_policy[\"best_rho\"][0])\n",
    "# set the policy\n",
    "policy = LinearGaussianPolicy(parameters=np.array(np.split(best_thetas, env_eval.action_dim)),\n",
    "            dim_state=env_eval.state_dim,\n",
    "            dim_action=env_eval.action_dim)\n",
    "# make the policy deterministic\n",
    "policy.std_dev = 0\n",
    "policy.sigma_noise = 0"
   ],
   "id": "4bb108600d81ee58",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T17:49:35.636808Z",
     "start_time": "2024-10-14T17:49:32.033345Z"
    }
   },
   "cell_type": "code",
   "source": "frames = evaluate(env_eval, policy)",
   "id": "d6ade5c94c173355",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -0.0003710601368786612 Std reward: 0.021104402599125775\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T17:49:37.533182Z",
     "start_time": "2024-10-14T17:49:36.789815Z"
    }
   },
   "cell_type": "code",
   "source": "create_video(frames, 60, \"/Users/leonardo/Desktop/Thesis/MagicRL/videos/hopper\")",
   "id": "cf1daf51916ddad8",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "16a43d180d8c2801"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
